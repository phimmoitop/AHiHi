name: AHiHi to CF (AI Anime Master)

on:
  workflow_dispatch:
    inputs:
      type: { description: "Type", required: true }
      slug: { description: "Slug", required: true }
      ss: { description: "Season", required: true }
      ep: { description: "Ep", required: true }
      hid: { description: "Hid", required: true }
      token: { description: 'Token', required: true }
      page: { description: 'Page', required: true }
      account: { description: 'AID', required: true }
      key: { description: 'Gemini API Key', required: true }

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Environment
        run: |
          sudo apt update
          sudo apt install -y ffmpeg curl jq python3 python3-pip
          python3 -m pip install --upgrade pip
          python3 -m pip install edge-tts webvtt-py google-genai

      - name: Get download and subtitle link
        id: getlink
        run: |
          HID="${{ github.event.inputs.hid }}"
          EP="${{ github.event.inputs.ep }}"
          RESP1=$(curl -s "https://hihianimeapi.onrender.com/api/v1/episodes/a-${HID}")
          LINK_EP=$(echo "$RESP1" | jq -r ".data[] | select(.episodeNumber == ($EP|tonumber)) | .id")
          RESP2=$(curl -s "https://hihianimeapi.onrender.com/api/v1/stream?type=sub&server=hd-2&id=${LINK_EP}")
          LINK_DL=$(echo "$RESP2" | jq -r '.data.link.file')
          LINK_SUB=$(echo "$RESP2" | jq -r '.data.tracks[]? | select(.label == "English") | .file' | head -n 1)
          if [ -z "$LINK_SUB" ] || [ "$LINK_SUB" == "null" ]; then
            LINK_SUB=$(echo "$RESP2" | jq -r '.data.tracks[0].file')
          fi
          echo "LINK_DL=$LINK_DL" >> $GITHUB_ENV
          echo "LINK_SUB=$LINK_SUB" >> $GITHUB_ENV

      - name: Download video and subtitle
        run: |
          ffmpeg -i "$LINK_DL" -c copy -bsf:a aac_adtstoasc "raw_video.mp4"
          curl -L "$LINK_SUB" -o "eng.vtt"

      - name: AI Contextual Translation & TTS
        env:
          GEMINI_API_KEY: ${{ github.event.inputs.key }}
        run: |
          python3 <<EOF
          import webvtt
          from google import genai
          import os, asyncio, edge_tts, time, re

          client = genai.Client(api_key=os.environ['GEMINI_API_KEY'])

          async def process():
              if not os.path.exists('eng.vtt'): return
              vtt = webvtt.read('eng.vtt')
              total = len(vtt)
              if not os.path.exists('chunks'): os.makedirs('chunks')

              batch_size = 30
              all_translated_texts = [None] * total
              print(f"ðŸš€ AI Ä‘ang dá»‹ch {total} cÃ¢u...")

              for i in range(0, total, batch_size):
                  batch = vtt[i:i+batch_size]
                  en_text = "\n".join([f"{idx+i}: {cap.text}" for idx, cap in enumerate(batch)])
                  prompt = f"Dá»‹ch sang tiáº¿ng Viá»‡t phong cÃ¡ch Anime Fantasy (VÆ°Æ¡ng Quá»‘c, Ma VÆ°Æ¡ng). DÃ¹ng Ta-NgÆ°Æ¡i cho Ä‘á»‘i Ä‘áº§u, Anh-Em cho thÃ¢n máº­t. Äá»‹nh dáº¡ng 'ID: Ná»™i dung'. KhÃ´ng giáº£i thÃ­ch.\n\n{en_text}"
                  try:
                      response = client.models.generate_content(model="gemini-1.5-flash", contents=prompt)
                      for line in response.text.strip().split('\n'):
                          match = re.match(r'^(\d+):\s*(.*)', line)
                          if match:
                              idx = int(match.group(1))
                              if idx < total: all_translated_texts[idx] = match.group(2).replace('*', '').strip()
                  except Exception as e:
                      print(f"Lá»—i AI táº¡i batch {i}: {e}")
                  time.sleep(1)

              vi_vtt = webvtt.WebVTT()
              for i, caption in enumerate(vtt):
                  text_vi = all_translated_texts[i] if all_translated_texts[i] else caption.text
                  vi_vtt.captions.append(webvtt.Caption(caption.start, caption.end, text_vi))
                  chunk_path = f"chunks/c_{i}.mp3"
                  try:
                      communicate = edge_tts.Communicate(text_vi, "vi-VN-HoaiMyNeural")
                      await communicate.save(chunk_path)
                  except:
                      os.system(f"ffmpeg -f lavfi -i anullsrc=r=24000:cl=mono -t 0.1 {chunk_path} -y")

              vi_vtt.save('vi.vtt')
          asyncio.run(process())
          EOF

      - name: Build Unified Filter Script
        run: |
          python3 <<EOF
          import webvtt, os
          if not os.path.exists('vi.vtt'):
              print("Lá»—i: KhÃ´ng tÃ¬m tháº¥y vi.vtt"); exit(1)
          
          vtt = webvtt.read('vi.vtt')
          with open('master_filter.txt', 'w') as f:
              # 1. Delay tá»«ng chunk
              for i, cap in enumerate(vtt):
                  h, m, s = cap.start.replace(',', '.').split(':')
                  ms = int(float(h)*3600000 + float(m)*60000 + float(s)*1000)
                  # Sá»­a lá»—i cÃº phÃ¡p f-string á»Ÿ Ä‘Ã¢y
                  f.write(f"[{i+1}]:a adelay={ms}|{ms} [a{i}];")
              
              # 2. Mix audio theo batch 20 cÃ¢u Ä‘á»ƒ trÃ¡nh lá»—i FFmpeg amix limit
              batch_size = 20
              labels = [f"[a{i}]" for i in range(len(vtt))]
              intermediate_mixes = []
              for i in range(0, len(labels), batch_size):
                  batch = labels[i:i+batch_size]
                  out_label = f"[m{i}]"
                  f.write(f"{''.join(batch)}amix=inputs={len(batch)}:dropout_transition=0 {out_label};")
                  intermediate_mixes.append(out_label)
              
              # Mix cuá»‘i
              f.write(f"{''.join(intermediate_mixes)}amix=inputs={len(intermediate_mixes)}:dropout_transition=0,volume=3.5[tts_only];")
              f.write(f"[0:a]volume=0.6[bg_music];[bg_music][tts_only]amix=inputs=2:duration=first[out_audio];")
              f.write(f"[0:v]subtitles='vi.vtt':force_style='FontSize=20,PrimaryColour=&H00FFFF&,Outline=1,Shadow=1'[out_video]")
          EOF

      - name: Final Render & Accurate HLS
        run: |
          NAME="${{ github.event.inputs.type }}-${{ github.event.inputs.slug }}-${{ github.event.inputs.ss }}-${{ github.event.inputs.ep }}"
          INPUTS="-i raw_video.mp4"
          for f in $(ls -v chunks/c_*.mp3); do INPUTS="$INPUTS -i $f"; done

          ffmpeg -y $INPUTS -filter_complex_script master_filter.txt \
            -map "[out_video]" -map "[out_audio]" \
            -c:v libx264 -preset superfast -crf 22 \
            -force_key_frames "expr:gte(t,n_forced*5)" \
            -c:a aac -b:a 128k "final_video.mp4"

          mkdir -p output
          ffmpeg -i final_video.mp4 -c copy \
            -f hls -hls_time 5 -hls_list_size 0 \
            -hls_playlist_type vod -hls_segment_type fmp4 \
            -hls_fmp4_init_filename "${NAME}-index.png" \
            -hls_segment_filename "output/${NAME}-index%d.png" \
            "output/${NAME}.m3u8"
          
          cp vi.vtt output/

      - name: Deploy to Cloudflare Pages
        uses: cloudflare/pages-action@v1
        with:
          apiToken: ${{ github.event.inputs.token }}
          accountId: ${{ github.event.inputs.account }}
          projectName: ${{ github.event.inputs.page }}
          directory: ./output
          branch: ${{ github.event.inputs.type }}-${{ github.event.inputs.slug }}-${{ github.event.inputs.ss }}-${{ github.event.inputs.ep }}
